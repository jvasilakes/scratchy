{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Generic Backprop\n",
    "\n",
    "The previous notebook, which constructed a neural network with a single hidden layer and a corresponding backprop training function, gave us an idea of how we might generalize these implementations to build and train neural networks of arbitrary width and depth.\n",
    "\n",
    "In **Part 1**, we'll generalize our model implementation to allow us to easily build fully-connected layers and chain them together into a model. \n",
    "\n",
    "In **Part 2**, we'll generalize our backprop implementation to work with these models without having to explicitly hand-code the chain rule multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: A generalized model API\n",
    "\n",
    "   The first thing we'll need is a way to refer to a given function (e.g. softmax or binary cross-entropy) and its derivative in a clear way. We'll do that with a `struct` named `Operation` below. Note that this is just a restructuring of the function definitions from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Operation(\"BinaryCrossEntropy\", binary_crossentropy, d_binary_crossentropy)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll need our clip function again to avoid domain errors when using log.\n",
    "function clip(x::Real, low=1e-15, hi=1 - 1e-15)\n",
    "    if x < low\n",
    "        return low\n",
    "    elseif x > hi\n",
    "        return hi\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "end\n",
    "\n",
    "# We'll name it \"Operation\" because \"Function\" is already taken by Julia.\n",
    "struct Operation\n",
    "    name::String  # The name of this operation\n",
    "    forward::Function  # The function\n",
    "    backward::Function # The derivative of this function\n",
    "end\n",
    "\n",
    "# Sigmoid activation\n",
    "function sigmoid(h::Array{Float64})\n",
    "    return @. 1.0 / (1.0 + exp(-h))\n",
    "end\n",
    "\n",
    "function d_sigmoid(h::Array{Float64})\n",
    "    forward_vals = sigmoid(h)\n",
    "    return @. forward_vals * (1.0 - forward_vals)\n",
    "end\n",
    "\n",
    "Sigmoid = Operation(\"Sigmoid\", sigmoid, d_sigmoid)\n",
    "\n",
    "# Binary cross entropy\n",
    "function binary_crossentropy(y::Array, o::Array{Float64})\n",
    "    # y: the gold-standard labels 0 or 1\n",
    "    # o: the real-valued output [0,1]\n",
    "    n = size(o, 1)\n",
    "    y_hat = clip.(o)\n",
    "    ces = @. y * log(o) + (1 - y) * log(1 - o)\n",
    "    return -(1 / n) * sum(ces)\n",
    "end\n",
    "\n",
    "function d_binary_crossentropy(y::Array, o::Array{Float64})\n",
    "    # y: the gold-standard labels 0 or 1\n",
    "    # o: the real-valued output [0,1]\n",
    "    y_hat = clip.(o)\n",
    "    return @. (o - y) / (o * (1 - o))\n",
    "end\n",
    "\n",
    "BinaryCrossEntropy = Operation(\"BinaryCrossEntropy\", binary_crossentropy,\n",
    "                               d_binary_crossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try these out to make sure they work as expected. We'll use the same values as in the first notebook to check our answers against. As a reminder, these are\n",
    "\n",
    "$z = \\begin{pmatrix} 0.7 \\\\ 0.8 \\\\ 0.4 \\\\ 0.3 \\end{pmatrix}$\n",
    "\n",
    "$y = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} $\n",
    "\n",
    "$o = \\sigma(z) = \\begin{pmatrix} 0.67 \\\\ 0.69 \\\\ 0.60 \\\\ 0.57 \\end{pmatrix}$\n",
    "\n",
    "\n",
    "$\\frac{d{o}}{d{z}} = \\frac{d{\\sigma}}{d{z}} = \\sigma(z)(1-\\sigma(z)) =\n",
    "    \\begin{pmatrix} 0.22 \\\\ 0.21 \\\\ 0.24 \\\\ 0.24 \\end{pmatrix}\n",
    "$\n",
    "\n",
    "$ C(y, o) = 0.635 $\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{o}} = \\frac{o - y}{o(1-o)} =\n",
    "    \\begin{pmatrix} -1.50 \\\\ -1.45 \\\\ 2.49 \\\\ 2.35 \\end{pmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o = σ(z): [0.6681877721681662, 0.6899744811276125, 0.598687660112452, 0.574442516811659]\n",
      "do = dσ(z): [0.22171287329310904, 0.2139096965202944, 0.24026074574152914, 0.24445831169074586]\n",
      "C: 0.6352869781437197\n",
      "∂C: [-1.4925373134328357, -1.4492753623188408, 2.5, 2.3474178403755865]\n"
     ]
    }
   ],
   "source": [
    "z = [0.7, 0.8, 0.4, 0.3]\n",
    "println(\"o = σ(z): $(Sigmoid.forward(z))\")\n",
    "println(\"do = dσ(z): $(Sigmoid.backward(z))\")\n",
    "\n",
    "y = [1, 1, 0, 0]\n",
    "o = [0.67, 0.69, 0.60, 0.574]\n",
    "println(\"C: $(BinaryCrossEntropy.forward(y, o))\")\n",
    "println(\"∂C: $(BinaryCrossEntropy.backward(y, o))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Layer\n",
    "\n",
    "A neural network layer is simply a matrix multiplication plus the bias followed by an activation function, i.e. $\\sigma(XW + b)$. Thus we'll need variables for $\\sigma$, $W$, and $b$. We'll also include placeholders for the weighted inputs $z = WX + b$ and the activations $o = \\sigma(z)$, since we'll need these in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Random\n",
    "Random.seed!(0)\n",
    "\n",
    "mutable struct Layer\n",
    "    name::String            # What we'll call this layer.\n",
    "    W::Array{Float64}       # The weight matrix\n",
    "    b::Array{Float64}       # The bias term\n",
    "    σ::Operation            # The activation function\n",
    "    δ::Array{Float64}       # Used in backprop\n",
    "    input::Array{Float64}   # The inputs from the previous layer\n",
    "    z::Array{Float64}       # Weighted inputs XW + b\n",
    "    output::Array{Float64}  # The output of this layer, i.e. the actvations\n",
    "\n",
    "    # Constructor v1: When we have already initialized W and b\n",
    "    function Layer(name::String, W::Array{Float64}, b::Array{Float64}, σ::Operation)\n",
    "        if !isa(σ, Operation)\n",
    "            error(\"Activation function '$σ' in layer '$name' is not an Operation\")\n",
    "        end\n",
    "        if size(W, 2) != size(b, 2)\n",
    "            error(\"Incompatible W and b shapes in layer '$name'\")\n",
    "        else\n",
    "            # Since δ, input, and output are computed later, we init as empty arrays.\n",
    "            new(name, W, b, σ, [], [], [], [])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Constructor v2: When we want to randomly initialize W and b given the shape of W.\n",
    "    function Layer(name::String, size::Tuple{Int,Int}, σ::Operation)\n",
    "        W = Random.rand(Float64, size)\n",
    "        b = Random.rand(Float64, (1, size[2]))\n",
    "        # Since δ, input, and output are computed later, we init as empty arrays.\n",
    "        new(name, W, b, σ, [], [], [], [])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try building a Layer using both constructors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer1 W: [0.8236475079774124 0.16456579813368521; 0.9103565379264364 0.17732884646626457], (2, 2)\n",
      "Layer1 b: [0.278880109331201 0.20347655804192266], (1, 2)\n",
      "Layer2 W: [0.042301665932029664 0.3618283907762174; 0.06826925550564478 0.9732164043865108], (2, 2)\n",
      "Layer2 b: [0.5858115517433242 0.5392892841426182], (1, 2)\n"
     ]
    }
   ],
   "source": [
    "W = Random.rand(Float64, (2, 2))\n",
    "b = Random.rand(Float64, (1, 2))\n",
    "layer1 = Layer(\"Layer1\", W, b, Sigmoid)\n",
    "println(\"Layer1 W: $(layer1.W), $(size(layer1.W))\")\n",
    "println(\"Layer1 b: $(layer1.b), $(size(layer1.b))\")\n",
    "\n",
    "layer2 = Layer(\"Layer2\", (2, 2), Sigmoid)\n",
    "println(\"Layer2 W: $(layer2.W), $(size(layer2.W))\")\n",
    "println(\"Layer2 b: $(layer2.b), $(size(layer2.b))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer forward\n",
    "\n",
    "We'll now write a function that propagates some inputs $X$ through a layer by doing the matrix multiplication, adding the bias, and applying the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The ! after a function name in Julia means that the function has a \"side-effect\".\n",
    "# Here, forward! modifies the Layer by storing the values of X, z, and the output.\n",
    "function forward!(l::Layer, X::Array)\n",
    "    # It's always a good idea to double check that the shapes are compatible.\n",
    "    if size(X, 2) != size(l.W, 1)\n",
    "        error(\"Input shape $(size(X)) incompatible with layer '$(l.name)' shape $(size(l.W))\")\n",
    "    end\n",
    "    l.input = X\n",
    "    l.z = (X * l.W) .+ l.b  # Weighted inputs\n",
    "    l.output = l.σ.forward(l.z)\n",
    "    return l.output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try running some data through a Layer. We'll compute everything by hand here first, and then check the function's output against it.\n",
    "\n",
    "$ X = \\begin{pmatrix} 1 & 3 \\\\ 2 & 3 \\\\ 2 & 1 \\\\ 1 & 1 \\end{pmatrix} $\n",
    "\n",
    "$ W = \\begin{pmatrix} 0.1 \\\\ 0.2 \\end{pmatrix} $\n",
    "\n",
    "$ b = 0.1 $\n",
    "\n",
    "$ z = XW + b = \\begin{pmatrix} .8 \\\\ .9 \\\\ .5 \\\\ .3 \\end{pmatrix} $\n",
    "\n",
    "$ o = \\sigma(z) = \\begin{pmatrix} .69 \\\\ .71 \\\\ .62 \\\\ .57 \\end{pmatrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×2 Array{Int64,2}:\n",
       " 1  3\n",
       " 2  3\n",
       " 2  1\n",
       " 1  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.1\n",
       " 0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1-element Array{Float64,1}:\n",
       " 0.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Layer(\"test_layer1\", [0.1, 0.2], [0.1], Operation(\"Sigmoid\", sigmoid, d_sigmoid), Float64[], Float64[], Float64[], Float64[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [1 3; 2 3; 2 1; 1 1]\n",
    "display(X)\n",
    "W = [0.1, 0.2]\n",
    "display(W)\n",
    "b = [0.1]\n",
    "display(b)\n",
    "test_layer1 = Layer(\"test_layer1\", W, b, Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.6899744811276125\n",
       " 0.7109495026250039\n",
       " 0.6224593312018546\n",
       " 0.598687660112452"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.6899744811276125\n",
       " 0.7109495026250039\n",
       " 0.6224593312018546\n",
       " 0.598687660112452"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4×2 Array{Float64,2}:\n",
       " 1.0  3.0\n",
       " 2.0  3.0\n",
       " 2.0  1.0\n",
       " 1.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.8\n",
       " 0.9\n",
       " 0.5\n",
       " 0.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o = forward!(test_layer1, X)\n",
    "\n",
    "# These two should be equal\n",
    "display(o)\n",
    "display(test_layer1.output)\n",
    "\n",
    "display(test_layer1.input)\n",
    "display(test_layer1.z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first Network\n",
    "\n",
    "Looks good! Let's create another `struct` called `Network`, which will allow us to connect an arbitrary number of layers together. In the constructor, we'll iterate through the layers in order, checking that the shape of the previous layer's output is compatible with the shape of the current layer's input for the matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Network\n",
    "    name::String\n",
    "    layers::Array{Layer}\n",
    "    loss::Operation\n",
    "    η::Real  # The learning rate\n",
    "    \n",
    "    # Constructor: Check that all layer shapes are compatible\n",
    "    function Network(name::String, layers::Array{Layer}, loss::Operation, η::Real)\n",
    "        l1 = layers[1]\n",
    "        for l2 in layers[2:end]\n",
    "            l1_outdim = size(l1.W, 2)\n",
    "            l2_indim = size(l2.W, 1)\n",
    "            if l1_outdim != l2_indim\n",
    "                error(\"\"\"Output dimension $(size(l1.W, 2)) of layer\n",
    "                      '$(l1.name)' incompatible with input dimension\n",
    "                      $(size(l2.W, 1)) of layer '$(l2.name)'.\"\"\")\n",
    "            end\n",
    "        end\n",
    "        new(name, layers, loss, η)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The forward pass\n",
    "\n",
    "We have a `forward!` function for a Layer, but to implement the forward pass through the entire network we'll have to string these together. This is pretty simple: the input to `forward!` for `Layer` $l$ is simply the output of `forward!` from `Layer` $l-1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 2 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The forward! function applied to a Network applies the forword! function to each layer.\n",
    "# Julia allows duplicate function names as long as the argument signatures are unique.\n",
    "function forward!(network::Network, X::Array)\n",
    "    layer_input = X\n",
    "    for l in network.layers\n",
    "        layer_input = forward!(l, layer_input)\n",
    "    end\n",
    "    return layer_input\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out our new Network by passing the XOR data through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\"FFNN\", Layer[Layer(\"layer1\", [0.8236475079774124 0.16456579813368521 0.278880109331201; 0.9103565379264364 0.17732884646626457 0.20347655804192266], [0.042301665932029664 0.06826925550564478 0.3618283907762174], Operation(\"Sigmoid\", sigmoid, d_sigmoid), Float64[], Float64[], Float64[], Float64[]), Layer(\"layer2\", [0.9732164043865108; 0.5858115517433242; 0.5392892841426182], [0.26003585026904785], Operation(\"Sigmoid\", sigmoid, d_sigmoid), Float64[], Float64[], Float64[], Float64[])], Operation(\"BinaryCrossEntropy\", binary_crossentropy, d_binary_crossentropy), 0.1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Random\n",
    "Random.seed!(0)\n",
    "\n",
    "X = [0 0; 1 1; 0 1; 1 0]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "layer1 = Layer(\"layer1\", (2, 3), Sigmoid)\n",
    "layer2 = Layer(\"layer2\", (3, 1), Sigmoid)\n",
    "\n",
    "nn = Network(\"FFNN\", [layer1, layer2], BinaryCrossEntropy, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0.7986272513484474; 0.8607659784098839; 0.83684593664789; 0.8355006815768814]\n",
      "Loss: 0.9830090442849736\n"
     ]
    }
   ],
   "source": [
    "o = forward!(nn, X)\n",
    "loss = nn.loss.forward(y, o)\n",
    "println(\"Output: $o\")\n",
    "println(\"Loss: $loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: A generalized backward pass\n",
    "\n",
    "In the previous notebook, we casually introduced the $\\delta$ term when computing the gradient of the final layer.\n",
    "\n",
    "$\\delta^2 = \\frac{\\partial{C}}{\\partial{o^2}} \\frac{d{o^2}}{d{z^2}}$\n",
    "\n",
    "We also casually defined an analogous term for the first layer.\n",
    "\n",
    "$\\delta^1 = \\delta^2 {W^2}^T \\odot \\frac{d{o^1}}{d{z^1}}$\n",
    "\n",
    "A keen eye may notice a pattern. Specifically, the $\\delta$ value of a given layer can be defined recursively in terms of the $\\delta$ value of the subsequent layer.\n",
    "\n",
    "$\\delta^l = \\delta^{l+1} {W^{(l+1)}}^T \\odot \\frac{d{o^l}}{d{z^l}}$\n",
    "\n",
    "The base case of this recursion (i.e. the $\\delta$ of the last layer) is the partial derivative of the loss wrt the output activations.\n",
    "\n",
    "$\\delta^L = \\frac{\\partial{C}}{\\partial{o^L}} \\frac{d{o^L}}{d{z^L}}$\n",
    "\n",
    "Practically, we'll compute $\\delta$ using a dynamic programming approach, rather than pure recursion. We know that our base case is always the last layer of the network, so all we have to do is iterate backwards through the layers. We implement this computation below in `backward!` using the helper function `compute_δ`, which takes care of the base case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_δ(network::Network, y::Array, i::Int)\n",
    "    l = network.layers[i]\n",
    "    dσ_dz = l.σ.backward(l.z)\n",
    "    # The base case of the recursion: the last layer in the network.\n",
    "    if i == length(network.layers)\n",
    "        o = network.layers[i].output\n",
    "        δ = network.loss.backward(y, o) .* dσ_dz\n",
    "        return δ\n",
    "    end\n",
    "    l_next = network.layers[i+1]\n",
    "    # l_next.δ should have already been computed because we're iterating\n",
    "    # backwards through the network's layers in backward!() below.\n",
    "    δ = (l_next.δ * transpose(l_next.W)) .* dσ_dz\n",
    "    return δ\n",
    "end\n",
    "\n",
    "function backward!(net::Network, y::Array)\n",
    "    num_layers = length(net.layers)\n",
    "    for i in num_layers:-1:1  # Iterate backwards through the network\n",
    "        l = net.layers[i]\n",
    "        l.δ = compute_δ(net, y, i)\n",
    "        ∂C_∂W = transpose(l.δ) * l.input\n",
    "        l.W = l.W .- (net.η * transpose(∂C_∂W))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "\n",
    "These new `forward!` and `backward!` functions significantly simplify the training loop compared to the previous notebooks, as we no longer have to hand-code the computations. All we have to do is call the functions in turn for a number of training steps. We can also wrap the training loop into a convenient `fit!` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fit!(net::Network, X::Array, y::Array, epochs::Int)\n",
    "    for i=1:epochs\n",
    "        o = forward!(net, X)\n",
    "        backward!(net, y)\n",
    "        if i == 1 || i % 100 == 0\n",
    "            loss = net.loss.forward(y, o)\n",
    "            println(\"Loss ($i): $(loss)\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function decision(o)\n",
    "    return Int.(o .>= 0.5)\n",
    "end\n",
    "\n",
    "function accuracy(y, y_hat)\n",
    "    return sum(y .== y_hat) / size(y, 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (1): 0.8440679451088628\n",
      "Loss (100): 0.6917856093020156\n",
      "Loss (200): 0.6791466562783384\n",
      "Loss (300): 0.6223753322078728\n",
      "Loss (400): 0.4704569715575505\n",
      "Loss (500): 0.2592341316199555\n",
      "Loss (600): 0.15583218836661303\n",
      "Loss (700): 0.10859790064025662\n",
      "Loss (800): 0.08283359752023395\n",
      "Loss (900): 0.06683245627560258\n",
      "Loss (1000): 0.05598357684820689\n",
      "Logits: [0.08510592053629093; 0.025215539501435614; 0.9542861963945827; 0.9396069522452936]\n",
      "Predictions: [0; 0; 1; 1]\n",
      "Gold Labels: [0, 0, 1, 1]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create the training dataset\n",
    "X = [0 0; 1 1; 0 1; 1 0]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "# Instantiate the network\n",
    "layer1 = Layer(\"layer1\", (2, 3), Sigmoid)\n",
    "layer2 = Layer(\"layer2\", (3, 1), Sigmoid)\n",
    "nn = Network(\"FFNN\", [layer1, layer2], BinaryCrossEntropy, 0.2)\n",
    "\n",
    "# Fit the network to the training data\n",
    "fit!(nn, X, y, 1000)\n",
    "\n",
    "# Compute the accuracy of the network on the training data.\n",
    "logits = forward!(nn, X)\n",
    "predictions = decision(logits)\n",
    "acc = accuracy(y, predictions)\n",
    "println(\"Logits: $(logits)\")\n",
    "println(\"Predictions: $(predictions)\")\n",
    "println(\"Gold Labels: $(y)\")\n",
    "println(\"Accuracy: $(acc)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensibility\n",
    "\n",
    "Our generic model API allows us to easily implement extensions. For example, we implement two additional activations functions, ReLU and Tanh, below.\n",
    "\n",
    "#### ReLU: Rectified Linear Unit\n",
    "$$ \\text{ReLU}(x) = max\\{0, x\\} $$\n",
    "\n",
    "$$ \\frac{d}{dx} \\text{ReLU}(x) = max\\{0, 1\\} $$\n",
    "\n",
    "\n",
    "#### Tanh\n",
    "$$ \\text{tanh}(x) = \\frac{sinh(x)}{cosh(x)} = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "which is just a scaled and shifted form of the sigmoid function over $[-1, 1]$.\n",
    "\n",
    "$$ \\frac{d}{dx} \\text{tanh}(x) = 1 - \\text{tanh}(x)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Operation(\"Tanh\", tanh, d_tanh)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function relu(x)\n",
    "    return replace(a -> a<0 ? 0 : a, x)\n",
    "end\n",
    "\n",
    "function d_relu(x)\n",
    "    return replace(a -> a<0 ? 0 : 1, x)\n",
    "end\n",
    "\n",
    "ReLU = Operation(\"ReLU\", relu, d_relu)\n",
    "\n",
    "\n",
    "function tanh(x)\n",
    "    ex = exp.(x)\n",
    "    enx = exp.(-x)\n",
    "    return @. (ex - enx) / (ex + enx)\n",
    "end\n",
    "\n",
    "function d_tanh(x)\n",
    "    return @. 1 - (tanh(x)^2)\n",
    "end\n",
    "\n",
    "Tanh = Operation(\"Tanh\", tanh, d_tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our API, we can just plug these new activation functions into a model, call `fit!`, and we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\"FFNN2\", Layer[Layer(\"layer1\", [0.06684644402498341 0.6052967398293401 0.838117753907359; 0.15663663731366406 0.13574455851185352 0.9147120238969264], [0.30007495800798534 0.7228497594213787 0.1196525672223625], Operation(\"Tanh\", tanh, d_tanh), Float64[], Float64[], Float64[], Float64[]), Layer(\"layer2\", [0.7670696322682211 0.48466052213279887 0.8011185163108001; 0.8019235854122897 0.8991991479715158 0.12432272872023531; 0.035344549147287685 0.951690700362799 0.11426876182995338], [0.07955447119057157 0.7766742218683131 0.1048226490565447], Operation(\"ReLU\", relu, d_relu), Float64[], Float64[], Float64[], Float64[]), Layer(\"layer3\", [0.8380749803307581; 0.18411485558080476; 0.3121451099216308], [0.19640742703220093], Operation(\"Sigmoid\", sigmoid, d_sigmoid), Float64[], Float64[], Float64[], Float64[])], Operation(\"BinaryCrossEntropy\", binary_crossentropy, d_binary_crossentropy), 0.1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [0 0; 1 1; 0 1; 1 0]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "layer1 = Layer(\"layer1\", (2, 3), Tanh)\n",
    "layer2 = Layer(\"layer2\", (3, 3), ReLU)\n",
    "layer3 = Layer(\"layer3\", (3, 1), Sigmoid)\n",
    "loss_fn = BinaryCrossEntropy\n",
    "η = 0.1\n",
    "\n",
    "nn = Network(\"FFNN2\", [layer1, layer2, layer3], loss_fn, η)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (1): 0.9850234463314683\n",
      "Loss (100): 0.6835836632884462\n",
      "Loss (200): 0.17327416700322357\n",
      "Loss (300): 0.010846782680794621\n",
      "Loss (400): 0.0047282465220858435\n",
      "Loss (500): 0.002922212896037504\n",
      "Loss (600): 0.0021234994460337816\n",
      "Loss (700): 0.0016600812448748644\n",
      "Loss (800): 0.00135598481721898\n",
      "Loss (900): 0.0011420860955858095\n",
      "Loss (1000): 0.000983656142416299\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "fit!(nn, X, y, 1000)\n",
    "predictions = decision(forward!(nn, X))\n",
    "println(\"Accuracy: $(accuracy(y, predictions))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it would be nice to summarize a network without seeing all the individual weights. Let's write a function to do that, which overloads the built-in `display`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network: FFNN2\n",
      "layer1 : (2, 3) : Tanh\n",
      "layer2 : (3, 3) : ReLU\n",
      "layer3 : (3, 1) : Sigmoid\n",
      "Loss: BinaryCrossEntropy\n"
     ]
    }
   ],
   "source": [
    "import Base.display\n",
    "\n",
    "function display(nn::Network)\n",
    "    println(\"Network: $(nn.name)\")\n",
    "    for layer in nn.layers\n",
    "        println(\"$(layer.name) : $(size(layer.W)) : $(layer.σ.name)\")\n",
    "    end\n",
    "    println(\"Loss: $(nn.loss.name)\")\n",
    "end\n",
    "\n",
    "display(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
