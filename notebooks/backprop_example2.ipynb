{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A less basic backprop example\n",
    "\n",
    "The last notebook walked through an implementation of backpropagation for a small linear network with a single weight matrix and no bias. It ended with an example problem, *XOR*, that linear networks aren't able to solve. Here, we'll introduce a single hidden layer to this network to increase its expressivity and allow it solve the XOR problem. \n",
    "\n",
    "As part of this process, we'll work towards the implemention of a more generalizable backprop training loop.\n",
    "\n",
    "First, though, let's go over the problem again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR\n",
    "\n",
    "Exclusive OR, or XOR, is a logical operation on two inputs which evaluates to 1 if the inputs are different, and 0 if they are the same. Formally, \n",
    "\n",
    "$XOR(a, b) = \\begin{cases}\n",
    "    1, & \\text{if}~~~ a \\neq b \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "This problem is not linearly separable, in that it is impossible to draw a straight line separating the inputs which evaluate to 1 from those which evaluate to 0. See the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots; pyplot()\n",
    "\n",
    "X = [0 0; 1 1; 0 1; 1 0]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "scatter(X[:, 1], X[:, 2], color=[:blue, :blue, :red, :red], markersize=10, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four points $X$ and their corresponding XOR value $y$ will be our dataset.\n",
    "\n",
    "$X = \\begin{pmatrix} 0 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$\n",
    "\n",
    "$y = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "Now, we need to extend our previous linear model to include a hidden layer. Practically, this means two weight matrices, $W^1$ and $W^2$, where the superscript denotes the layer number. Like last time, we have two *features* as input. Rather than going from 2 features to 1 output node, however, we'll go from 2 features to 3 hidden nodes, and then from the 3 hidden nodes to 1 output node. For simplicity, both layers will use the sigmoid activation function.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "*N.B. it is sufficient to have just 2 hidden nodes to solve this problem, but using 3 helps us keep the orientation of our matrices straight and is helpful for instruction.*\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Putting this together, we have our model.\n",
    "\n",
    "$o^1 = \\sigma(XW^1)$\n",
    "\n",
    "$o^2 = \\sigma(o^1 W^2)$\n",
    "\n",
    "where\n",
    "\n",
    "$X \\in \\mathbb{R}^{4 \\times 2}$ the input data\n",
    "\n",
    "$W^1 \\in \\mathbb{R}^{2 \\times 3}$ the hidden layer weights\n",
    "\n",
    "$W^2 \\in \\mathbb{R}^{3 \\times 1}$ the output weights\n",
    "\n",
    "$\\sigma(z)= \\frac{1}{1 + e^{-z}}$  the sigmoid activation function\n",
    "\n",
    "\n",
    "We define and initialize the weights and sigmoid function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "σ (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = [0.1 0.1 0.1; 0.1 0.1 0.1]\n",
    "W2 = [0.1, 0.1, 0.1]\n",
    "\n",
    "function σ(z)\n",
    "    # Sigmoid activation function\n",
    "    return @. 1 / (1 + exp(-z))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward pass\n",
    "\n",
    "We start by computing the hidden layer output value, $o^1$.\n",
    "\n",
    "$z^1 = XW^1 = \n",
    "    \\begin{pmatrix} 0 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \n",
    "    \\begin{pmatrix} 0.1 & 0.1 & 0.1 \\\\ 0.1 & 0.1 & 0.1 \\end{pmatrix}\n",
    "    = \n",
    "    \\begin{pmatrix} 0 & 0 & 0 \\\\ 0.2 & 0.2 & 0.2 \\\\ 0.1 & 0.1 & 0.1 \\\\ 0.1 & 0.1 & 0.1 \\end{pmatrix}$\n",
    "    \n",
    "$o^1 = \\sigma(z^1) = \\begin{pmatrix} \n",
    "                0.5 & 0.5 & 0.5 \\\\\n",
    "                0.55 & 0.55 & 0.55 \\\\\n",
    "                0.52 & 0.52 & 0.52 \\\\\n",
    "                0.52 & 0.52 & 0.52\n",
    "               \\end{pmatrix}$\n",
    "\n",
    "Then, the network output $o^2$.\n",
    "               \n",
    "$z^2 = o^1 W^2 = \n",
    "    \\begin{pmatrix} 0.5 & 0.5 & 0.5 \\\\0.55 & 0.55 & 0.55 \\\\ 0.52 & 0.52 & 0.52 \\\\ 0.52 & 0.52 & 0.52 \\end{pmatrix} \n",
    "    \\begin{pmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\end{pmatrix}\n",
    "    = \n",
    "    \\begin{pmatrix} 0.15 \\\\ 0.165 \\\\ 0.156 \\\\ 0.156 \\end{pmatrix}\n",
    "$\n",
    "\n",
    "$o^2 = \\sigma(z^2) =\n",
    "    \\begin{pmatrix} 0.537 \\\\ 0.541 \\\\ 0.539 \\\\ 0.539 \\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1: [0.0 0.0 0.0; 0.2 0.2 0.2; 0.1 0.1 0.1; 0.1 0.1 0.1]\n",
      "o1: [0.5 0.5 0.5; 0.549833997312478 0.549833997312478 0.549833997312478; 0.52497918747894 0.52497918747894 0.52497918747894; 0.52497918747894 0.52497918747894 0.52497918747894]\n",
      "z2: [0.15000000000000002, 0.1649501991937434, 0.157493756243682, 0.157493756243682]\n",
      "o2: [0.5374298453437496, 0.5411443022794791, 0.5392922545992791, 0.5392922545992791]\n"
     ]
    }
   ],
   "source": [
    "z1 = X * W1\n",
    "o1 = σ(z1)\n",
    "z2 = o1 * W2\n",
    "o2 = σ(z2)\n",
    "\n",
    "println(\"z1: $z1\")\n",
    "println(\"o1: $o1\")\n",
    "println(\"z2: $z2\")\n",
    "println(\"o2: $o2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply our decision function to the values of $o^2$ to get the predicted $y$ values $\\hat{y}$ and compute the accuracy of the model. As a reminder, our decision function is a simple threshold\n",
    "\n",
    "$\\hat{y} = \\begin{cases}\n",
    "    1, & \\text{if}~~~ o \\ge 0.5 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "The relevant functions and the output of our initial model are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "function decision(o)\n",
    "    return Int.(o .>= 0.5)\n",
    "end\n",
    "\n",
    "function accuracy(y, y_hat)\n",
    "    N = size(y, 1)\n",
    "    return sum(y .== y_hat) / N\n",
    "end\n",
    "\n",
    "y_hat = decision(o2)\n",
    "acc = accuracy(y, y_hat)\n",
    "println(y)\n",
    "println(y_hat)\n",
    "println(\"Accuracy: $acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the loss of our initial model. Since this is still a binary classification problem, we'll use binary cross entropy again.\n",
    "\n",
    "$C = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\times ln(o_i) + (1 - y_i) \\times ln(1 - o_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.6962429568948773\n"
     ]
    }
   ],
   "source": [
    "function clip(a, lo=1e-15, hi=1 - 1e-15)\n",
    "    if a < lo\n",
    "        return lo\n",
    "    elseif a > hi\n",
    "        return hi\n",
    "    else\n",
    "        return a\n",
    "    end\n",
    "end\n",
    "\n",
    "function binary_crossentropy(y, o)\n",
    "    N = size(y, 1)\n",
    "    # We \"clip\" the predicted values to avoid\n",
    "    # domain errors in the log() function if our\n",
    "    # model happens to predict 0.0, and to prevent\n",
    "    # log returning 0.0 if the model predicts 1.0\n",
    "    o = clip.(o)\n",
    "    ces = @. (y * log(o)) + ((1-y) * log(1-o))\n",
    "    return (-1/N) * sum(ces)\n",
    "end\n",
    "\n",
    "loss = binary_crossentropy(y, o2)\n",
    "println(\"Initial loss: $loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The backward pass\n",
    "\n",
    "Okay, the easy part is over. Now we have to compute the partial derivatives of each of the weights (from both weight matrices) with respect to the loss. We'll go through the equations at a high level first, so we can see how they fit together, then we'll get down to computing exactly what everything is.\n",
    "\n",
    "The partial wrt $W^2$ is analogous to the partial we computed for the linear model. That is,\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^2}} =\n",
    "    \\frac{\\partial{C}}{\\partial{o^2}}\n",
    "    \\frac{d{o^2}}{d{z^2}}\n",
    "    \\frac{\\partial{z^2}}{\\partial{W^2}}\n",
    "$\n",
    "\n",
    "The partial wrt $W^1$ is a bit more involved,\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^1}} =\n",
    "    \\frac{\\partial{C}}{\\partial{o^2}}\n",
    "    \\frac{d{o^2}}{d{z^2}}\n",
    "    \\frac{\\partial{z^2}}{\\partial{o^1}}\n",
    "    \\frac{d{o^1}}{d{z^1}}\n",
    "    \\frac{\\partial{z^1}}{\\partial{W^1}}\n",
    "$\n",
    "\n",
    "We can simplify this a bit, however, by noticing that the first two terms of $\\frac{\\partial{C}}{\\partial{W^2}}$, namely $\\frac{\\partial{C}}{\\partial{o^2}} \\frac{d{o^2}}{d{z^2}}$, occur in both equations. We'll assign this operation the name $\\delta^2$.\n",
    "\n",
    "$\\delta^2 = \\frac{\\partial{C}}{\\partial{o^2}} \\frac{d{o^2}}{d{z^2}}$\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^2}} =\n",
    "    \\delta^2\n",
    "    \\frac{\\partial{z^2}}{\\partial{W^2}}\n",
    "$\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^1}} =\n",
    "    \\delta^2\n",
    "    \\frac{\\partial{z^2}}{\\partial{o^1}}\n",
    "    \\frac{d{o^1}}{d{z^1}}\n",
    "    \\frac{\\partial{z^1}}{\\partial{W^1}}\n",
    "$\n",
    "\n",
    "Before we go any further, let's look at the shapes. First for the forward pass:\n",
    "\n",
    "$X \\in \\mathbb{R}^{4 \\times 2}$\n",
    "\n",
    "$W^1 \\in \\mathbb{R}^{2 \\times 3}$\n",
    "\n",
    "$z^1 = XW^1 \\in \\mathbb{R}^{4 \\times 3}$\n",
    "\n",
    "$o^1 = \\sigma(z^1) \\in \\mathbb{R}^{4 \\times 3}$\n",
    "\n",
    "$W^2 \\in \\mathbb{R}^{3 \\times 1}$\n",
    "\n",
    "$z^2 = o^1W^2 \\in \\mathbb{R}^{4 \\times 1}$\n",
    "\n",
    "$o^2 = \\sigma(z^2) \\in \\mathbb{R}^{4 \\times 1}$\n",
    "\n",
    "Then the backward pass:\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{o^2}} \\in \\mathbb{R}^{4 \\times 1}$ (same shape as $o^2$)\n",
    "\n",
    "$\\frac{d{o^2}}{d{z^2}} \\in \\mathbb{R}^{4 \\times 1}$ (same shape as $z^2$)\n",
    "\n",
    "$\\frac{\\partial{z^2}}{\\partial{W^2}} = o^1 \\in \\mathbb{R}^{4 \\times 3}$\n",
    "\n",
    "$\\frac{\\partial{z^2}}{\\partial{o^1}} = W^2 \\in \\mathbb{R}^{3 \\times 1}$\n",
    "\n",
    "$\\frac{d{o^1}}{d{z^1}} \\in \\mathbb{R}^{4 \\times 3}$ (same shape as $z^1$)\n",
    "\n",
    "$\\frac{\\partial{z^1}}{\\partial{W^1}} = X \\in \\mathbb{R}^{4 \\times 2}$\n",
    "\n",
    "### Output layer weights $W^2$\n",
    "---\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^2}} =\n",
    "    \\frac{\\partial{C}}{\\partial{o^2}}\n",
    "    \\frac{d{o^2}}{d{z^2}}\n",
    "    \\frac{\\partial{z^2}}{\\partial{W^2}}\n",
    "$\n",
    "\n",
    "Now the shapes of the multiplications. First off, $\\frac{\\partial{C}}{\\partial{o^2}}$ and $\\frac{d{o^2}}{d{z^2}}$ have the same shape, so we want to compute their Hadamard product:\n",
    "\n",
    "$\\delta^2 = \\frac{\\partial{C}}{\\partial{o^2}} \\frac{d{o^2}}{d{z^2}} = \\mathbb{R}^{4 \\times 1} \\odot \\mathbb{R}^{4 \\times 1} = \\mathbb{R}^{4 \\times 1}$\n",
    "\n",
    "But $\\frac{\\partial{z^2}}{\\partial{W^2}} = o^1 \\in \\mathbb{R}^{4 \\times 3}$ so we'll need to transpose $\\delta^2$ to get a compatible shape.\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^2}} = {\\delta^2}^T \\frac{\\partial{z^2}}{\\partial{W^2}} = \\mathbb{R}^{1 \\times 4} \\mathbb{R}^{4 \\times 3} = \\mathbb{R}^{1 \\times 3}$\n",
    "\n",
    "which transposed is $\\mathbb{R}^{3 \\times 1}$, so its compatible with $W^2$. This is encouraging, but let's make sure it makes sense by replacing the dimensions with variables indicating their meaning.\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^2}} = {\\delta^2}^T \\frac{\\partial{z^2}}{\\partial{W^2}} \\in \\mathbb{R}^{O \\times N} \\mathbb{R}^{N \\times H} = \\mathbb{R}^{O \\times H}$\n",
    "\n",
    "$W^2 \\in \\mathbb{R}^{H ~\\times~ O}$\n",
    "\n",
    "where $N$ is the number of examples, $H$ is the number of hidden nodes, and $O$ is the number of output nodes. \n",
    "\n",
    "Looking at the equations this way, we can see that $\\frac{\\partial{C}}{\\partial{W^2}}$ connects the output nodes to the hidden nodes, and that transposing the result does indeed match up the proper dimensions to $W^2$ to allow us to do the weight updates for this layer. Great!\n",
    "\n",
    "\n",
    "### Hidden layer weights $W^1$\n",
    "---\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^1}} =\n",
    "    \\delta^2\n",
    "    \\frac{\\partial{z^2}}{\\partial{o^1}}\n",
    "    \\frac{d{o^1}}{d{z^1}}\n",
    "    \\frac{\\partial{z^1}}{\\partial{W^1}}\n",
    "$\n",
    "\n",
    "Next up, $\\frac{\\partial{C}}{\\partial{W^1}}$. We know already that $\\delta^2 \\in \\mathbb{R}^{4 \\times 1}$ and $\\frac{\\partial{z^2}}{\\partial{o^1}} = W^2 \\in \\mathbb{R}^{3 \\times 1}$, so \n",
    "\n",
    "$\\delta^2 \\frac{\\partial{z^2}}{\\partial{o^1}} =\n",
    "    \\delta^2 {\\frac{\\partial{z^2}}{\\partial{o^1}}}^T =\n",
    "    \\delta^2 {W^2}^T \\in \\mathbb{R}^{4 \\times 1} \\mathbb{R}^{1 \\times 3} =\n",
    "    \\mathbb{R}^{4 \\times 3}\n",
    "$\n",
    "\n",
    "$\\frac{d{o^1}}{d{z^1}} \\in \\mathbb{R}^{4 \\times 3}$ so we take the Hadamard product again.\n",
    "\n",
    "$\\delta^2 {W^2}^T \\odot \\frac{d{o^1}}{d{z^1}} = \\mathbb{R}^{4 \\times 3} \\mathbb{R}^{4 \\times 3} = \\mathbb{R}^{4 \\times 3}$\n",
    "\n",
    "For concision's sake we'll define this as another $\\delta$\n",
    "\n",
    "$\\delta^1 = \\delta^2 {W^2}^T \\odot \\frac{d{o^1}}{d{z^1}}$\n",
    "\n",
    "Finally, $\\frac{\\partial{z^1}}{\\partial{W^1}} = X \\in \\mathbb{R}^{4 \\times 1}$ so transposing the previous we get\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^1}} = {\\delta^1}^T X = \\mathbb{R}^{3 \\times 4} \\mathbb{R}^{4 \\times 2} = \\mathbb{R}^{3 \\times 2}$\n",
    "\n",
    "Looking at the shapes as our variables again\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^1}} = {\\delta^1}^T X = \\mathbb{R}^{H \\times N} \\mathbb{R}^{N \\times I} = \\mathbb{R}^{H \\times I}$ where $I$ is the number of input nodes.\n",
    "\n",
    "So transposing this we get $\\mathbb{R}^{I \\times H}$ which is the shape of $W^1$ and we're all set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dσ_dz (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ∂C_∂o(y, o)\n",
    "    num = o .- y\n",
    "    denom = o .* (1 .- o)\n",
    "    return num ./ denom\n",
    "end\n",
    "\n",
    "function dσ_dz(z)\n",
    "    o = σ(z)\n",
    "    return @. o * (1.0 - o)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step 1: Loss: 0.6962429568948773, Accuracy: 0.5\n",
      "Training step 10: Loss: 0.693802702686104, Accuracy: 0.5\n",
      "Training step 20: Loss: 0.6932639114382373, Accuracy: 0.5\n",
      "Training step 30: Loss: 0.6931679623020855, Accuracy: 0.5\n",
      "Training step 40: Loss: 0.6931508790702442, Accuracy: 0.5\n",
      "Training step 50: Loss: 0.6931478376644078, Accuracy: 0.5\n",
      "Training step 60: Loss: 0.693147296198902, Accuracy: 0.5\n",
      "Training step 70: Loss: 0.6931471998017269, Accuracy: 0.5\n",
      "Training step 80: Loss: 0.6931471826401677, Accuracy: 0.5\n",
      "Training step 90: Loss: 0.6931471795848999, Accuracy: 0.5\n",
      "Training step 100: Loss: 0.6931471790409676, Accuracy: 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.09884752543810783 0.09884752543810783 0.09884752543810783; 0.09884752543810783 0.09884752543810783 0.09884752543810783], [9.054280533571092e-5, 9.054280533571092e-5, 9.054280533571092e-5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(X, y, W1, W2, η=1, epochs=50, verbose=0)\n",
    "    log_at = epochs / 10\n",
    "    W1_t = copy(W1)\n",
    "    W2_t = copy(W2)\n",
    "    N = size(X, 1)\n",
    "    for t=1:epochs\n",
    "        # Forward pass\n",
    "        z1 = X * W1_t\n",
    "        o1 = σ(z1)\n",
    "        z2 = o1 * W2_t\n",
    "        o2 = σ(z2)\n",
    "        # Print the loss and training set accuracy every log_at epochs.\n",
    "        if t == 1 || t % log_at == 0\n",
    "            loss = binary_crossentropy(y, o2)\n",
    "            acc = accuracy(y, decision(o2))\n",
    "            println(\"Training step $t: Loss: $loss, Accuracy: $acc\")\n",
    "            if verbose > 0\n",
    "                println(\"W1: $W1_t\")\n",
    "                println(\"W2: $W2_t\")\n",
    "                println(\"---\")\n",
    "            end\n",
    "        end\n",
    "        # Backward pass\n",
    "        # Compute the gradients wrt W2 and W1\n",
    "        δ2 = ∂C_∂o(y, o2) .* dσ_dz(z2)\n",
    "        ∂C_∂W2 = transpose(δ2) * o1\n",
    "        δ1 = (δ2 * transpose(W2_t)) .* dσ_dz(z1)\n",
    "        ∂C_∂W1 = transpose(δ1) * X\n",
    "        # Update the weights given the gradients\n",
    "        W2_t = W2_t .- (η * transpose(∂C_∂W2))\n",
    "        W1_t = W1_t .- (η * transpose(∂C_∂W1))\n",
    "    end\n",
    "    return (W1_t, W2_t)  # return the trained weights.\n",
    "end\n",
    "\n",
    "train(X, y, W1, W2, 0.1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging\n",
    "\n",
    "What's going on here? We can see the loss is decreasing over the training steps, but incredibly slowly. Let's take advantage of our `verbose` option to get some more details. We'll also train for 1000 iterations just to make sure our model doesn't just need more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step 1: Loss: 0.6962429568948773, Accuracy: 0.5\n",
      "W1: [0.1 0.1 0.1; 0.1 0.1 0.1]\n",
      "W2: [0.1, 0.1, 0.1]\n",
      "---\n",
      "Training step 100: Loss: 0.6931471790409676, Accuracy: 0.5\n",
      "W1: [0.09884751723491753 0.09884751723491753 0.09884751723491753; 0.09884751723491753 0.09884751723491753 0.09884751723491753]\n",
      "W2: [9.215436848794269e-5, 9.215436848794269e-5, 9.215436848794269e-5]\n",
      "---\n",
      "Training step 200: Loss: 0.6931471789230962, Accuracy: 0.5\n",
      "W1: [0.09884818756232329 0.09884818756232329 0.09884818756232329; 0.09884818756232329 0.09884818756232329 0.09884818756232329]\n",
      "W2: [7.266565377296576e-5, 7.266565377296576e-5, 7.266565377296576e-5]\n",
      "---\n",
      "Training step 300: Loss: 0.693147178923033, Accuracy: 0.5\n",
      "W1: [0.09884883722741662 0.09884883722741662 0.09884883722741662; 0.09884883722741662 0.09884883722741662 0.09884883722741662]\n",
      "W2: [7.266355056038948e-5, 7.266355056038948e-5, 7.266355056038948e-5]\n",
      "---\n",
      "Training step 400: Loss: 0.6931471789229696, Accuracy: 0.5\n",
      "W1: [0.09884948690956519 0.09884948690956519 0.09884948690956519; 0.09884948690956519 0.09884948690956519 0.09884948690956519]\n",
      "W2: [7.26649323189193e-5, 7.26649323189193e-5, 7.26649323189193e-5]\n",
      "---\n",
      "Training step 500: Loss: 0.6931471789229062, Accuracy: 0.5\n",
      "W1: [0.09885013661247695 0.09885013661247695 0.09885013661247695; 0.09885013661247695 0.09885013661247695 0.09885013661247695]\n",
      "W2: [7.266631476169072e-5, 7.266631476169072e-5, 7.266631476169072e-5]\n",
      "---\n",
      "Training step 600: Loss: 0.6931471789228429, Accuracy: 0.5\n",
      "W1: [0.09885078633615375 0.09885078633615375 0.09885078633615375; 0.09885078633615375 0.09885078633615375 0.09885078633615375]\n",
      "W2: [7.266769726565198e-5, 7.266769726565198e-5, 7.266769726565198e-5]\n",
      "---\n",
      "Training step 700: Loss: 0.6931471789227795, Accuracy: 0.5\n",
      "W1: [0.0988514360805968 0.0988514360805968 0.0988514360805968; 0.0988514360805968 0.0988514360805968 0.0988514360805968]\n",
      "W2: [7.266907983068669e-5, 7.266907983068669e-5, 7.266907983068669e-5]\n",
      "---\n",
      "Training step 800: Loss: 0.6931471789227164, Accuracy: 0.5\n",
      "W1: [0.09885208584580729 0.09885208584580729 0.09885208584580729; 0.09885208584580729 0.09885208584580729 0.09885208584580729]\n",
      "W2: [7.267046245676702e-5, 7.267046245676702e-5, 7.267046245676702e-5]\n",
      "---\n",
      "Training step 900: Loss: 0.693147178922653, Accuracy: 0.5\n",
      "W1: [0.09885273563178638 0.09885273563178638 0.09885273563178638; 0.09885273563178638 0.09885273563178638 0.09885273563178638]\n",
      "W2: [7.267184514387065e-5, 7.267184514387065e-5, 7.267184514387065e-5]\n",
      "---\n",
      "Training step 1000: Loss: 0.6931471789225897, Accuracy: 0.5\n",
      "W1: [0.09885338543853531 0.09885338543853531 0.09885338543853531; 0.09885338543853531 0.09885338543853531 0.09885338543853531]\n",
      "W2: [7.267322789206993e-5, 7.267322789206993e-5, 7.267322789206993e-5]\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.09885339193670768 0.09885339193670768 0.09885339193670768; 0.09885339193670768 0.09885339193670768 0.09885339193670768], [7.267324171985885e-5, 7.267324171985885e-5, 7.267324171985885e-5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(X, y, W1, W2, 0.1, 1000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with 1000 iterations, the loss never decreases much and our weights don't seem to be diverging at all. To see why this might be happening, let's go back and compute the gradient wrt $W^2$ by hand.\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^2}} =\n",
    "    \\delta^2\n",
    "    \\frac{\\partial{z^2}}{\\partial{W^2}}\n",
    "$\n",
    "\n",
    "where $\\delta^2 = \\frac{\\partial{C}}{\\partial{o^2}} \\frac{d{o^2}}{d{z^2}}$\n",
    "\n",
    "Incidentally, in the case of binary cross entropy loss with sigmoid activation, the equation for computing $\\delta^2$ simplifies nicely.\n",
    "\n",
    "$\\delta^2 = \\frac{\\partial{C}}{\\partial{o^2}} \\frac{d{o^2}}{d{z^2}} =\n",
    "    \\frac{o^2-y}{o^2(1-o^2)} \\sigma(z^2)(1 - \\sigma(z^2)) = o^2 - y$\n",
    "    \n",
    "So\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^2}} = (o^2 - y)^T o^1 = \n",
    "    \\begin{pmatrix} .537 & .541 & .539 & .539 \\end{pmatrix}\n",
    "    \\begin{pmatrix} \n",
    "                0.5 & 0.5 & 0.5    \\\\\n",
    "                0.55 & 0.55 & 0.55 \\\\\n",
    "                0.52 & 0.52 & 0.52 \\\\\n",
    "                0.52 & 0.52 & 0.52\n",
    "    \\end{pmatrix} = \n",
    "    \\begin{pmatrix} 1.13 & 1.13 & 1.13 \\end{pmatrix}\n",
    "$\n",
    "\n",
    "Thus the equal updates for each weight seemed to be caused by the equal columns in $o^1$. More specifically, since $o^1 \\in \\mathbb{R}^{N \\times H}$ where $H$ is the number of hidden layer nodes, this seems to be caused by the uniform initialization of the weight matrices.\n",
    "\n",
    "<br>\n",
    "\n",
    "This issue is discussed in section 8.4 of the book [*Deep Learning*](http://www.deeplearningbook.org/) by Goodfellow et al.\n",
    "\n",
    "> Perhaps the only property known with complete certainty is that the initial\n",
    "parameters need to “break symmetry” between diﬀerent units. If two hidden\n",
    "units with the same activation function are connected to the same inputs, then\n",
    "these units must have diﬀerent initial parameters. If they have the same initial\n",
    "parameters, then a deterministic learning algorithm applied to a deterministic cost\n",
    "and model will constantly update both of these units in the same way.\n",
    "\n",
    "That sounds like our problem! They go on to say shortly after,\n",
    "\n",
    "> The goal of having each unit compute a diﬀerent function\n",
    "motivates random initialization of the parameters.\n",
    "\n",
    "So let's randomly initialize our weight matrices to see if that improves things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step 1: Loss: 0.7018173714043544, Accuracy: 0.5\n",
      "Training step 100: Loss: 0.6929179271263748, Accuracy: 0.5\n",
      "Training step 200: Loss: 0.6917059232072245, Accuracy: 0.75\n",
      "Training step 300: Loss: 0.6894707276242513, Accuracy: 0.75\n",
      "Training step 400: Loss: 0.6840029905634678, Accuracy: 0.75\n",
      "Training step 500: Loss: 0.6700648276853042, Accuracy: 0.75\n",
      "Training step 600: Loss: 0.641536953778409, Accuracy: 0.75\n",
      "Training step 700: Loss: 0.6007825192459852, Accuracy: 0.75\n",
      "Training step 800: Loss: 0.5568627051358982, Accuracy: 0.75\n",
      "Training step 900: Loss: 0.5152595851648379, Accuracy: 1.0\n",
      "Training step 1000: Loss: 0.4767706052143706, Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([3.9300606373175824 0.42170418803112797 0.8086009346238632; 3.9318161533300677 0.7969517262018306 0.40703864746177376], [4.720048654830316; -3.280291197339681; -2.9946374643525773])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Random\n",
    "Random.seed!(0)\n",
    "W1_ = Random.rand(Float64, (2, 3))\n",
    "W2_ = Random.rand(Float64, (3, 1))\n",
    "W1_trained, W2_trained = train(X, y, W1_, W2_, 0.1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1 Array{Int64,2}:\n",
       " 1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(X, W1, W2)\n",
    "    z1 = X * W1\n",
    "    o1 = σ(z1)\n",
    "    z2 = o1 * W2\n",
    "    o2 = σ(z2)\n",
    "    return decision(o2)\n",
    "end\n",
    "\n",
    "predict([1 0], W1_trained, W2_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that small tweak, we've solved the classic XOR problem!\n",
    "\n",
    "---\n",
    "\n",
    "Continue on to the next notebook where we'll generalize the code we've implemented here so that we can build and train neural nets of arbitrary width and depth. This will involve writing some boilerplate code that builds the network as a directed graph of connected layers, a generalization of our training function to operate over this graph, as well as some additional code to let us work with larger datasets. Oh, and we'll finally add in the bias terms!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
